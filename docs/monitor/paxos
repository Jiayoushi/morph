An Overview of Morph's Paxos


Monitor implemented a simple multi-paxos to provide a consistent view of the object store
cluster map which is used to do consistent hashing. Note everything said below applies
only to morph's implementation of paxos, not in general terms.

When an oss (object store server) wants to add itself to the monitor, it invokes the AddOss
RPC. The monitor will ask paxos to replicate this command before applying it. Only the leader
monitor can propose values to the paxos, which avoids the paxos livelock problem.

A leader is picked by an implicit rule: the server that is alive and with the highest id is
the current leader. Every monitor is sending out hearbeats to let others know they are alive. 
This can be costly only if the number of monitors is too much. 

When paxos starts on a value, it will first check if there is any log that is accepted 
but not yet chosen. If so, Morph's paxos will propose that value first, which makes the 
changes of oss cluster monotonically (it's not possible for the cluster change to go 
from config#0 to config#2 without going to #1 first). After paxos gets a log index that 
has no previous accepted value, paxos sets that log entry to accepted with the given value
and starts proposing to all acceptors. After the majority replies, it checks if there is 
any value already accepted, if so, it will use that value instead. Then, the paxos will
send Accept RPC to all acceptors and wait for a majority of responses.
Only if no acceptors from the majority has a larger min_proposal is the value considered
to be chosen.

Leader is used to mitigate the livelock problem. A leader is chosen to be whoever that is 
alive and has the highest server id at the time. Even if there are more than one leader
exists, it will not be a problem since paxos allow this.

Log compaction, client protocol and cluster membership change of the monitor cluster itself 
and so on are not yet implemented. The two-phase approach can be optimized into one as well.


A summary of the Morph's paxos implementation.

State
  Persistent State on all servers:
    min_proposal:      the largest proposal number this server has seen
                       proposal_number = [32-bit max_round][32-bit server_id]
    log[]:             log entries; each entry contains command (accepted_value) 
                       for state machine, accepted_proposal when entry was received.
 
  Voaltile States on leaders:
    first_unchosen[]:  the first unchosen index for each other server

 
Prepare RPC
  Arguments:
    log_index:         index of the log entry that the proposer wants to propose a value
    proposal:          the proposal number for this log entry
  
  Results:
    accepted_proposal: the proposal number that has already been accepted for this log entry
    accepted_value:    the value that has already been accepted for this log entry
    
  Receiver Implemntation:
    1. If proposal > min_proposal,
       min_proposal = proposal.
    2. If the log entry is already accepted, return the (accepted_proposal, accepted_value).


Accept RPC
  Arguments:
    log_index:         index of the log entry this RPC targets
    proposal:          the proposal number for this log entry
    value:             the proposed value for this log entry
    first_unchosen:    index of the first log entry that has not been chosen     
    
  Results:
    min_proposal:      the minimum proposal number this server has seen so far
    first_unchosen:    the first unchosen index of this server

  Receiver Implementation:
    1. If proposal >= min_proposal then 
       accepted_proposal = min_proposal = proposal,
       accepted_value = value
    2. For any log < first_unchosen, set the log to chosen by setting accepted_value to max infinity.
    3. Return min_proposal, first_unchosen_index


Success RPC:
  Arguments:
    log_index:         index of the log entry this RPC targets
    value:             the chosen value for this log entry

  Results:
    first_unchosen:    the first unchosen index of this server

  Receiver Implementation:
    1. Set log[log_index] = value.
    2. Return first unchosen log's index.


Rules for servers:
  All Servers:
    If the leader timed out, choose the server with the highest id to be the leader. Note there is 
    no election process invovled.

  Leaders:
    If Prepare RPC returns any accepted_value, replace the value with accepted_value for highest
    accepted_proposal
    If Accept RPC returns any min_proposal that is larger than the proposal number sent out,
    restarts the 2-phase commit process with a new proposal number. 
    If Accept RPC returns a first_unchosen that is less than the leader's first_unchosen, then
    send a Success RPC for each log entry that is chosen in the leader but not in the acceptor.


Baisc Comparison to Raft Properties
  This is some very short analysis of paxos' reaction to these properties. The guarantees of
  these properties of paxos is only very briefly analyzed. So they are incomplete.

  Election Safety: 
    raft: most one leader can be elected in a given term.
    paxos: paxos can have many proposers. Leader is used as an optimization to mitigate the
           livelock problem. At any point in time, as long as there are live servers, there
           wil be leaders. But majority restriction still exist to ensure consistency.
  Leader Append-Only: 
    raft: a leader never overwrites or deletes entries in its log;
          it only appends new entries
    paxos: it's the same for paxos. If a log index has already an accepted_value, the leader will
           send PrepareRPC using that value, it will not overwrite value.
           And if there is any value that is not chosen, the leader will first try to replicate
           that value first. So it's also append only.
  Log Matching: 
    raft: if two logs contain an entry with the same index and term, then the logs are identical in all entries
          up through the given index.
    paxos: it's not the case for paxos. An acceptor that lags behind can receive a 2-phase commit for a log_index
           that is much larger than the last chosen log index. Which means there could be gaps between the 
           last chosen log and the current log index that is being proposed by the leader. But it's okay since
           paxos ensures consistency on each log entry level.
  Leader Completness:
    raft: if a log entry is committed in a given term, then that entry will be present in the logs
          of the leaders for all higher-numbered terms
    paxos: not the case for paxos. A server becomes a leader as long as it's alive and has the highest server id.
           So it's possible for an old leader comes back alive to be the new leader with missing comitted entries.
           But it's okay, for each unchosen index the leader will send PrepareRPC only to find it's already has
           an accepted value, so nothing will go wrong.
  State Matching Safety:
    raft: if a server has applied a log entry at a given index to its state machine, no other server
          will ever apply a different log entry for the same index.
    paxos: true, otherwise it's not even a consensus algorithm.





Q: When should a server starts sending out Accept to replicate chosen entries if it thinks
   it's the leader?
A: What happens if the server starts right away? If it knows the entry is chosen, it sends
   out Accept. And everything goes well. Even if an entry is previously chosen, and this
   server does not know, it's okay. Since it will do a two-phase only to chosen the same
   value again. If it does not, what is it waiting for? 

Q: What if leader A is the only one that knows an entry has been chosen and now is down?
A: It's okay.
   The leader B comes along and try to prepare another value. Suppose it does not know the
   value before, it will try to issue its own value. It will discover a value has already been
   accpeted so he will use that value instead. And finally the value will be chosen again.

Q: If there is a prepare request coming from a server that the local server belives is not the 
   leader, is it safe to ignore the request?
A: It's okay and required. 
   Suppose a server B lost connection to the leader A, then it think it's the leader and 
   proceed to do prepare. Other servers still have the connection to the leader A, if they
   do not reject the request, leader B's prepare requests will go through, and then leader A's
   request will come right afterwards. This is livelock situation we want to solve in the first
   place, so we need to turn down requests coming from leaders that is not considered to be
   the leader.

Q: If there is an Accept request coming from a server that the local server belives is not the
   leader, is it safe to ignore the request?
A: It's okay and required.
   There are two types of accept requests, one is to replicate the chosen entries. Another one
   is to ask other servers to accept. 
   Let's fisrt analyze the first type of Accept. It's better to not turn down the accept because
   since it's chosen, its proposal number will be +INFINITY, and its value can definitely
   be used. But what if we turn it down? Will the new leader send the same accept? Yes. But the
   new leader may not know the value is chosen, then it will do a two-phase commit. Only after
   the two-phase completes will the same accept ever be sent again. So it's again better to not
   turn down the request.
   Let's now analyze the second type of Accept. If we receive an accept from a server A that we
   do not think is the leader, we should just turn it down. If we think server D is the leader,
   and we have a (A, B, C) (D, E) network partition. It's okay for D, E to turn down the Accept,
   since ABC will still form a quorum. If network is good other than A lose connection to D,
   it's still okay to turn down. It's always safe to turn down. What if we let the Accept through? 
   It's okay. It will only be a problem if leader D now issue a prepare request to change the
   value of min_proposal. Then livelock may arise. So we need to turn it down!
